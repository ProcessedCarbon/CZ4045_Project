{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Sequence Tagging: NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unkno\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\unkno\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:62: UserWarning: Pandas requires version '1.3.4' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import InputLayer, TimeDistributed, SpatialDropout1D, Bidirectional\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Word\t\tMost similar word\tCosine similarity\n",
      "-----------------------------------------------------------------------\n",
      "student\t\tstudents  \t\t0.7294867038726807\n",
      "Apple\t\tApple_AAPL  \t\t0.7456986308097839\n",
      "apple\t\tapples  \t\t0.720359742641449\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "words = [\"student\", \"Apple\", \"apple\"]\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(\"Word\\t\\tMost similar word\\tCosine similarity\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "for word in words:\n",
    "    most_similar = w2v.most_similar(positive=[word])\n",
    "    print(f\"{word}\\t\\t{most_similar[0][0]}  \\t\\t{most_similar[0][1]}\")\n",
    "print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLL2003_dir = '../Datasets/CoNLL2003_dataset'\n",
    "train_dir = f'{CoNLL2003_dir}/eng.train'\n",
    "dev_dir =  f'{CoNLL2003_dir}/eng.testa'\n",
    "test_dir =  f'{CoNLL2003_dir}/eng.testb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_content(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            content = file.readlines()\n",
    "        file.close()\n",
    "    except Exception as e:\n",
    "        content = None\n",
    "        print(e)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def print_items(item):\n",
    "    for s in item: print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = import_content(train_dir)\n",
    "dev_content = import_content(dev_dir)\n",
    "test_content = import_content(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data by sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(content):\n",
    "    split_data = [c.split(' ') for c in content] if content != None else []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    words = []\n",
    "\n",
    "    for line in split_data:\n",
    "        # if end of a sentence\n",
    "        if line == ['\\n']:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            s_text  = line[0]\n",
    "            s_tag = line[-1].replace('\\n','')\n",
    "\n",
    "            sentence.append([s_text, s_tag]) \n",
    "            words.append([s_text, s_tag])\n",
    "    \n",
    "    sentences.append(sentence) # last item in content not new line so must add previous sentence manually after loop           \n",
    "\n",
    "    return sentences, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_tag(sentences):\n",
    "    text = []\n",
    "    tag = []\n",
    "    combined = []\n",
    "    sentence_count = 1\n",
    "\n",
    "    for s in sentences:\n",
    "        for w in s:\n",
    "            w_text  = w[0]\n",
    "            w_tag = w[-1].replace('\\n','')\n",
    "\n",
    "            text.append(w_text)\n",
    "            tag.append(w_tag)        \n",
    "            combined.append({\n",
    "                'sentence': sentence_count,\n",
    "                'text' : w_text,\n",
    "                'tag' : w_tag\n",
    "            })   \n",
    "        sentence_count+=1       \n",
    "    return text, tag, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_words = split_sentences(train_content)\n",
    "dev_sentences, dev_words = split_sentences(dev_content)\n",
    "test_sentences, test_words = split_sentences(test_content)\n",
    "\n",
    "train_text, train_tag, train_combined = split_text_tag(train_sentences)\n",
    "dev_text, dev_tag, dev_combined = split_text_tag(dev_sentences)\n",
    "test_text, test_tag, test_combined = split_text_tag(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_voc = np.unique(np.array(train_text))\n",
    "dev_voc = np.unique(np.array(dev_text))\n",
    "\n",
    "\n",
    "tag_set = np.unique(np.array(train_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.2 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the size (number of sentences) of the training, development and test file for CoNLL2003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences (training): 14987\n",
      "Number of sentences (dev): 3466\n",
      "Number of sentences (test): 3684\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences (training):\", len(train_sentences))\n",
    "print(\"Number of sentences (dev):\", len(dev_sentences))\n",
    "print(\"Number of sentences (test):\", len(test_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO, etc.) you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag set (BIO): ['B-LOC' 'B-MISC' 'B-ORG' 'I-LOC' 'I-MISC' 'I-ORG' 'I-PER' 'O']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tag set (BIO):\", tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qn 1.2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiple_ne_sentence(sentences):\n",
    "    for sentence in sentences:\n",
    "        ne_count = 0\n",
    "        for word_info in sentence:\n",
    "            if \"B-\" in word_info[-1]:\n",
    "                ne_count+=1\n",
    "        if ne_count == 2:\n",
    "            return sentence\n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Swiss', 'I-MISC'],\n",
       " ['Grand', 'B-MISC'],\n",
       " ['Prix', 'I-MISC'],\n",
       " ['World', 'B-MISC'],\n",
       " ['Cup', 'I-MISC'],\n",
       " ['cycling', 'O'],\n",
       " ['race', 'O'],\n",
       " ['on', 'O'],\n",
       " ['Sunday', 'O'],\n",
       " [':', 'O']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = get_multiple_ne_sentence(train_sentences)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_entities(sentence):\n",
    "    inside_tags = ['I-ORG', 'I-LOC', 'I-PER', 'I-MISC'] # Tags that require multiple words to form an entity\n",
    "    begin_tags = ['B-LOC', 'B-ORG', 'B-MISC'] # Tags that are single word entities\n",
    "    outside_tags = ['O']\n",
    "    entities = [] # all entities gotten from search\n",
    "    entity = [] # word group of current entity if any group tags encountered\n",
    "    \n",
    "    for c in sentence:\n",
    "        if (c['tag'] in begin_tags or c['tag'] in outside_tags or c['tag'] == '\\n') and len(entity) != 0:\n",
    "            entities.append(' '.join(entity))\n",
    "            entity = []\n",
    "        if c['tag'] in begin_tags or c['tag'] in inside_tags: \n",
    "            entity.append(c['text'])\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete named entities in the sentence: ['Swiss', 'Grand Prix', 'World Cup']\n"
     ]
    }
   ],
   "source": [
    "_,_,sentence_text_tag = split_text_tag([sentence])\n",
    "print(\"Complete named entities in the sentence:\", get_named_entities(sentence_text_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag-text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_combined)\n",
    "dev_df = pd.DataFrame(dev_combined)\n",
    "test_df = pd.DataFrame(test_combined)\n",
    "\n",
    "# path = '../Datasets/Processed/'\n",
    "# file_name = 'CoNLL2003_processed'\n",
    "# # Export DataFrame to a CSV file\n",
    "# df.to_csv(f'{path}{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load w2v models for train and dev\n",
    "\n",
    "path = '../Pretrained_Models/'\n",
    "\n",
    "train_w2v = Word2Vec.load('../Pretrained_Models/CONLL2003_pretrain.model')\n",
    "\n",
    "train_pretrained_weights = train_w2v.wv.vectors\n",
    "train_num_tokens, train_embedding_dim = train_pretrained_weights.shape\n",
    "\n",
    "word2idx = train_w2v.wv.key_to_index\n",
    "word2idx['<UNK>'] = word2idx[list(word2idx.keys())[-1]]+1\n",
    "word2idx['<PAD>'] = word2idx[list(word2idx.keys())[-1]]+1\n",
    "voc = train_voc\n",
    "voc = np.append(voc,'<UNK>')\n",
    "voc = np.append(voc,'<PAD>')\n",
    "\n",
    "tag2idx = {k: v for v, k in enumerate(tag_set)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_word2idx = dict(w2v.key_to_index)\n",
    "w2v_voc = w2v.index_to_key\n",
    "w2v_word2idx['<UNK>'] = w2v_word2idx[list(w2v_word2idx.keys())[-1]]+1\n",
    "w2v_word2idx['<PAD>'] = w2v_word2idx[list(w2v_word2idx.keys())[-1]]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for v in w2v_voc:\n",
    "  embeddings_index[v] =  w2v[v] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17724 words (5902 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2idx.items():    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector[0:50]\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_sequence(sentences):\n",
    "  sequence = []\n",
    "  sent_seq = []\n",
    "  for s in sentences:\n",
    "    for w in s:\n",
    "      if w[0] in word2idx.keys():\n",
    "        sent_seq.append(word2idx[w[0]])\n",
    "      else:\n",
    "        sent_seq.append(word2idx['<UNK>'])\n",
    "    sequence.append(sent_seq)\n",
    "    sent_seq = []\n",
    "  \n",
    "  return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = get_x_sequence(train_sentences)\n",
    "#x_train = pad_sequences(maxlen=embedding_dim, sequences=x_train, padding=\"post\", value=len(word2idx)-1)\n",
    "x_train = pad_sequences(maxlen=embedding_dim, sequences=x_train, padding=\"post\")\n",
    "\n",
    "y_train = [[tag2idx[w[1]] for w in s] for s in train_sentences]\n",
    "y_train = pad_sequences(maxlen=embedding_dim, sequences=y_train, padding=\"post\", value=tag2idx['O'])\n",
    "#y_train = pad_sequences(maxlen=embedding_dim, sequences=y_train, padding=\"post\")\n",
    "\n",
    "x_dev = get_x_sequence(dev_sentences)\n",
    "#x_dev = pad_sequences(maxlen=embedding_dim, sequences=x_dev, padding=\"post\", value=len(word2idx)-1)\n",
    "x_dev = pad_sequences(maxlen=embedding_dim, sequences=x_dev, padding=\"post\")\n",
    "\n",
    "y_dev = [[tag2idx[w[1]] for w in s] for s in dev_sentences]\n",
    "y_dev = pad_sequences(maxlen=embedding_dim, sequences=y_dev, padding=\"post\", value=tag2idx['O'])\n",
    "#y_dev = pad_sequences(maxlen=embedding_dim, sequences=y_dev, padding=\"post\")\n",
    "\n",
    "x_test = get_x_sequence(test_sentences)\n",
    "#x_dev = pad_sequences(maxlen=embedding_dim, sequences=x_dev, padding=\"post\", value=len(word2idx)-1)\n",
    "x_test = pad_sequences(maxlen=embedding_dim, sequences=x_test, padding=\"post\")\n",
    "\n",
    "y_test = [[tag2idx[w[1]] for w in s] for s in test_sentences]\n",
    "y_test = pad_sequences(maxlen=embedding_dim, sequences=y_test, padding=\"post\", value=tag2idx['O'])\n",
    "#y_test = pad_sequences(maxlen=embedding_dim, sequences=y_test, padding=\"post\")\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_dev = to_categorical(y_dev)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 50, 50)            1181400   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               13056     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 400)               102800    \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 50, 8)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1317456 (5.03 MB)\n",
      "Trainable params: 136056 (531.47 KB)\n",
      "Non-trainable params: 1181400 (4.51 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(tag_set)\n",
    "sequence_length = 50\n",
    "output_shape=(sequence_length,num_classes)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(InputLayer(embedding_dim))\n",
    "#model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim,  embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "#    trainable=False,))\n",
    "#model.add(SpatialDropout1D(0.01))\n",
    "#model.add(Bidirectional(LSTM(units=embedding_dim, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(Embedding(input_dim=num_tokens, output_dim=embedding_dim, input_length = sequence_length, trainable=False,))\n",
    "model.add(LSTM(units=sequence_length))\n",
    "model.add(Dense(units=256, activation= \"relu\", kernel_regularizer=regularizers.L1L2(l1=0.025, l2=0.025)))\n",
    "model.add(Dropout(0.01))  \n",
    "#model.add(Dense(units=256, activation= \"sigmoid\", kernel_regularizer=regularizers.L1L2(l1=0.0025, l2=0.0025)))\n",
    "#model.add(Dropout(0.01))  \n",
    "model.add(Dense(sequence_length * num_classes, activation='softmax'))\n",
    "model.add(tf.keras.layers.Reshape(output_shape))\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "early_stopping = EarlyStopping(patience=10)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "15/15 [==============================] - 3s 115ms/step - loss: 24.2098 - accuracy: 0.7891 - val_loss: 21.2331 - val_accuracy: 0.9415\n",
      "Epoch 2/1000\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 18.4657 - accuracy: 0.9485 - val_loss: 15.6712 - val_accuracy: 0.9510\n",
      "Epoch 3/1000\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 13.7213 - accuracy: 0.9546 - val_loss: 11.6970 - val_accuracy: 0.9510\n",
      "Epoch 4/1000\n",
      "15/15 [==============================] - 1s 91ms/step - loss: 10.1175 - accuracy: 0.9546 - val_loss: 8.4604 - val_accuracy: 0.9510\n",
      "Epoch 5/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 7.1722 - accuracy: 0.9546 - val_loss: 5.8330 - val_accuracy: 0.9510\n",
      "Epoch 6/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 4.8189 - accuracy: 0.9546 - val_loss: 3.7822 - val_accuracy: 0.9510\n",
      "Epoch 7/1000\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 3.0132 - accuracy: 0.9546 - val_loss: 2.2530 - val_accuracy: 0.9510\n",
      "Epoch 8/1000\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 1.7132 - accuracy: 0.9546 - val_loss: 1.2109 - val_accuracy: 0.9510\n",
      "Epoch 9/1000\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.8837 - accuracy: 0.9546 - val_loss: 0.6212 - val_accuracy: 0.9510\n",
      "Epoch 10/1000\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.4830 - accuracy: 0.9546 - val_loss: 0.4016 - val_accuracy: 0.9510\n",
      "Epoch 11/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.3298 - accuracy: 0.9546 - val_loss: 0.3049 - val_accuracy: 0.9510\n",
      "Epoch 12/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2720 - accuracy: 0.9546 - val_loss: 0.2809 - val_accuracy: 0.9510\n",
      "Epoch 13/1000\n",
      "15/15 [==============================] - 1s 97ms/step - loss: 0.2565 - accuracy: 0.9546 - val_loss: 0.2723 - val_accuracy: 0.9510\n",
      "Epoch 14/1000\n",
      "15/15 [==============================] - 1s 98ms/step - loss: 0.2502 - accuracy: 0.9546 - val_loss: 0.2691 - val_accuracy: 0.9510\n",
      "Epoch 15/1000\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.2492 - accuracy: 0.9546 - val_loss: 0.2677 - val_accuracy: 0.9510\n",
      "Epoch 16/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2470 - accuracy: 0.9546 - val_loss: 0.2666 - val_accuracy: 0.9510\n",
      "Epoch 17/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2460 - accuracy: 0.9546 - val_loss: 0.2654 - val_accuracy: 0.9510\n",
      "Epoch 18/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2451 - accuracy: 0.9546 - val_loss: 0.2649 - val_accuracy: 0.9510\n",
      "Epoch 19/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2449 - accuracy: 0.9546 - val_loss: 0.2639 - val_accuracy: 0.9510\n",
      "Epoch 20/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2435 - accuracy: 0.9546 - val_loss: 0.2630 - val_accuracy: 0.9510\n",
      "Epoch 21/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2437 - accuracy: 0.9546 - val_loss: 0.2634 - val_accuracy: 0.9510\n",
      "Epoch 22/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2422 - accuracy: 0.9546 - val_loss: 0.2611 - val_accuracy: 0.9510\n",
      "Epoch 23/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2425 - accuracy: 0.9546 - val_loss: 0.2626 - val_accuracy: 0.9510\n",
      "Epoch 24/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2415 - accuracy: 0.9546 - val_loss: 0.2610 - val_accuracy: 0.9510\n",
      "Epoch 25/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2417 - accuracy: 0.9546 - val_loss: 0.2618 - val_accuracy: 0.9510\n",
      "Epoch 26/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2436 - accuracy: 0.9546 - val_loss: 0.2623 - val_accuracy: 0.9510\n",
      "Epoch 27/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2416 - accuracy: 0.9546 - val_loss: 0.2614 - val_accuracy: 0.9510\n",
      "Epoch 28/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2424 - accuracy: 0.9546 - val_loss: 0.2620 - val_accuracy: 0.9510\n",
      "Epoch 29/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2414 - accuracy: 0.9546 - val_loss: 0.2608 - val_accuracy: 0.9510\n",
      "Epoch 30/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2419 - accuracy: 0.9546 - val_loss: 0.2612 - val_accuracy: 0.9510\n",
      "Epoch 31/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2410 - accuracy: 0.9546 - val_loss: 0.2610 - val_accuracy: 0.9510\n",
      "Epoch 32/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2417 - accuracy: 0.9546 - val_loss: 0.2612 - val_accuracy: 0.9510\n",
      "Epoch 33/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2409 - accuracy: 0.9546 - val_loss: 0.2601 - val_accuracy: 0.9510\n",
      "Epoch 34/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2413 - accuracy: 0.9546 - val_loss: 0.2605 - val_accuracy: 0.9510\n",
      "Epoch 35/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2407 - accuracy: 0.9546 - val_loss: 0.2605 - val_accuracy: 0.9510\n",
      "Epoch 36/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2411 - accuracy: 0.9546 - val_loss: 0.2596 - val_accuracy: 0.9510\n",
      "Epoch 37/1000\n",
      "15/15 [==============================] - 1s 92ms/step - loss: 0.2405 - accuracy: 0.9546 - val_loss: 0.2613 - val_accuracy: 0.9510\n",
      "Epoch 38/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2409 - accuracy: 0.9546 - val_loss: 0.2600 - val_accuracy: 0.9510\n",
      "Epoch 39/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2409 - accuracy: 0.9546 - val_loss: 0.2603 - val_accuracy: 0.9510\n",
      "Epoch 40/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2414 - accuracy: 0.9546 - val_loss: 0.2602 - val_accuracy: 0.9510\n",
      "Epoch 41/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2404 - accuracy: 0.9546 - val_loss: 0.2603 - val_accuracy: 0.9510\n",
      "Epoch 42/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2408 - accuracy: 0.9546 - val_loss: 0.2603 - val_accuracy: 0.9510\n",
      "Epoch 43/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2411 - accuracy: 0.9546 - val_loss: 0.2603 - val_accuracy: 0.9510\n",
      "Epoch 44/1000\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.2409 - accuracy: 0.9546 - val_loss: 0.2603 - val_accuracy: 0.9510\n",
      "Epoch 45/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2408 - accuracy: 0.9546 - val_loss: 0.2601 - val_accuracy: 0.9510\n",
      "Epoch 46/1000\n",
      "15/15 [==============================] - 1s 94ms/step - loss: 0.2410 - accuracy: 0.9546 - val_loss: 0.2606 - val_accuracy: 0.9510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x188f3d5e4c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 1000\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, shuffle=True, validation_data=(x_dev, y_dev),\n",
    "         callbacks = early_stopping, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 3ms/step\n",
      "[[[6.15498436e-07 5.82216671e-07 5.58322256e-07 ... 1.62139512e-03\n",
      "   8.70085030e-04 5.98045159e-03]\n",
      "  [3.87648561e-06 4.49454683e-06 5.40902988e-07 ... 7.84689386e-04\n",
      "   1.06217689e-03 7.01039424e-03]\n",
      "  [1.17958211e-06 6.08354355e-07 5.34358151e-07 ... 5.33043756e-04\n",
      "   8.11943668e-04 7.43896794e-03]\n",
      "  ...\n",
      "  [1.69001646e-06 1.67680071e-06 1.64462256e-06 ... 1.25277775e-05\n",
      "   1.78473674e-05 4.47417572e-02]\n",
      "  [1.82357815e-06 1.56989813e-06 1.54706765e-06 ... 6.67803488e-06\n",
      "   1.24740336e-05 4.55944128e-02]\n",
      "  [2.34641379e-06 2.11114525e-06 2.30248975e-06 ... 2.12997679e-06\n",
      "   1.74081561e-06 1.11727633e-01]]\n",
      "\n",
      " [[6.15498436e-07 5.82216671e-07 5.58322256e-07 ... 1.62139512e-03\n",
      "   8.70085030e-04 5.98045159e-03]\n",
      "  [3.87648561e-06 4.49454683e-06 5.40902988e-07 ... 7.84689386e-04\n",
      "   1.06217689e-03 7.01039424e-03]\n",
      "  [1.17958211e-06 6.08354355e-07 5.34358151e-07 ... 5.33043756e-04\n",
      "   8.11943668e-04 7.43896794e-03]\n",
      "  ...\n",
      "  [1.69001646e-06 1.67680071e-06 1.64462256e-06 ... 1.25277775e-05\n",
      "   1.78473674e-05 4.47417572e-02]\n",
      "  [1.82357815e-06 1.56989813e-06 1.54706765e-06 ... 6.67803488e-06\n",
      "   1.24740336e-05 4.55944128e-02]\n",
      "  [2.34641379e-06 2.11114525e-06 2.30248975e-06 ... 2.12997679e-06\n",
      "   1.74081561e-06 1.11727633e-01]]\n",
      "\n",
      " [[6.15498323e-07 5.82216558e-07 5.58322199e-07 ... 1.62139488e-03\n",
      "   8.70084914e-04 5.98045113e-03]\n",
      "  [3.87648515e-06 4.49454637e-06 5.40902931e-07 ... 7.84689269e-04\n",
      "   1.06217677e-03 7.01039331e-03]\n",
      "  [1.17958189e-06 6.08354242e-07 5.34358094e-07 ... 5.33043698e-04\n",
      "   8.11943552e-04 7.43896700e-03]\n",
      "  ...\n",
      "  [1.69001623e-06 1.67680048e-06 1.64462233e-06 ... 1.25277757e-05\n",
      "   1.78473638e-05 4.47417498e-02]\n",
      "  [1.82357780e-06 1.56989790e-06 1.54706743e-06 ... 6.67803397e-06\n",
      "   1.24740327e-05 4.55944054e-02]\n",
      "  [2.34641357e-06 2.11114502e-06 2.30248952e-06 ... 2.12997656e-06\n",
      "   1.74081538e-06 1.11727618e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[6.15498436e-07 5.82216671e-07 5.58322256e-07 ... 1.62139512e-03\n",
      "   8.70085030e-04 5.98045159e-03]\n",
      "  [3.87648561e-06 4.49454683e-06 5.40902988e-07 ... 7.84689386e-04\n",
      "   1.06217689e-03 7.01039424e-03]\n",
      "  [1.17958211e-06 6.08354355e-07 5.34358151e-07 ... 5.33043756e-04\n",
      "   8.11943668e-04 7.43896794e-03]\n",
      "  ...\n",
      "  [1.69001646e-06 1.67680071e-06 1.64462256e-06 ... 1.25277775e-05\n",
      "   1.78473674e-05 4.47417572e-02]\n",
      "  [1.82357815e-06 1.56989813e-06 1.54706765e-06 ... 6.67803488e-06\n",
      "   1.24740336e-05 4.55944128e-02]\n",
      "  [2.34641379e-06 2.11114525e-06 2.30248975e-06 ... 2.12997679e-06\n",
      "   1.74081561e-06 1.11727633e-01]]\n",
      "\n",
      " [[6.15498436e-07 5.82216671e-07 5.58322256e-07 ... 1.62139512e-03\n",
      "   8.70085030e-04 5.98045159e-03]\n",
      "  [3.87648561e-06 4.49454683e-06 5.40902988e-07 ... 7.84689386e-04\n",
      "   1.06217689e-03 7.01039424e-03]\n",
      "  [1.17958211e-06 6.08354355e-07 5.34358151e-07 ... 5.33043756e-04\n",
      "   8.11943668e-04 7.43896794e-03]\n",
      "  ...\n",
      "  [1.69001646e-06 1.67680071e-06 1.64462256e-06 ... 1.25277775e-05\n",
      "   1.78473674e-05 4.47417572e-02]\n",
      "  [1.82357815e-06 1.56989813e-06 1.54706765e-06 ... 6.67803488e-06\n",
      "   1.24740336e-05 4.55944128e-02]\n",
      "  [2.34641379e-06 2.11114525e-06 2.30248975e-06 ... 2.12997679e-06\n",
      "   1.74081561e-06 1.11727633e-01]]\n",
      "\n",
      " [[6.15498436e-07 5.82216671e-07 5.58322256e-07 ... 1.62139512e-03\n",
      "   8.70085030e-04 5.98045159e-03]\n",
      "  [3.87648561e-06 4.49454683e-06 5.40902988e-07 ... 7.84689386e-04\n",
      "   1.06217689e-03 7.01039424e-03]\n",
      "  [1.17958211e-06 6.08354355e-07 5.34358151e-07 ... 5.33043756e-04\n",
      "   8.11943668e-04 7.43896794e-03]\n",
      "  ...\n",
      "  [1.69001646e-06 1.67680071e-06 1.64462256e-06 ... 1.25277775e-05\n",
      "   1.78473674e-05 4.47417572e-02]\n",
      "  [1.82357815e-06 1.56989813e-06 1.54706765e-06 ... 6.67803488e-06\n",
      "   1.24740336e-05 4.55944128e-02]\n",
      "  [2.34641379e-06 2.11114525e-06 2.30248975e-06 ... 2.12997679e-06\n",
      "   1.74081561e-06 1.11727633e-01]]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = model.predict(x_test)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2315 - accuracy: 0.9565\n",
      "Test Loss: 0.23151466250419617, Test Accuracy: 0.9565255045890808\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  973, 12289,   237,   783,     5,  4433,   211,  6498,     0,\n",
       "        23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625,\n",
       "        23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625,\n",
       "        23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625,\n",
       "        23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625, 23625,\n",
       "        23625, 23625, 23625, 23625, 23625]),\n",
       " 50)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5, 7, 4, 7, 7, 7, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7]),\n",
       " 50)"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[i], len(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.1694952e-01, 4.9378670e-08, 2.4964581e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 4.4022022e-20, 7.8977473e-20, 1.3015374e-04,\n",
       "        1.1669247e-37, 2.9517738e-17, 6.3284081e-01, 3.5020872e-15,\n",
       "        4.2665351e-04, 9.5816960e-32, 1.2303811e-29, 4.0297813e-25,\n",
       "        0.0000000e+00, 9.8721791e-38, 0.0000000e+00, 3.9840713e-36,\n",
       "        6.9979742e-06, 1.3861541e-22, 3.6760278e-37, 1.6089272e-18,\n",
       "        1.5299677e-28, 4.0904094e-28, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.3691262e-28, 7.8219448e-29, 3.4032821e-17, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
       " 50)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_sentence, len(predicted_labels_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
